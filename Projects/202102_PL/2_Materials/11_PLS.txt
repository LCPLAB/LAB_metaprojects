隨著時代演進，人類快速的發展，人工智慧的發展也越來越進步，許多領域也開始應用AI來協助決策，像是企業運用AI來篩選合適的應徵者。但AI是否真能準確無誤地做出預測呢？還是它與我們一樣，容易被自己的想法所誤導，進而做出錯誤的決策？

黃從仁（2019）回顧了AI領域中機器學習的偏誤決策，機器學習雖然可以對人類的思想和行為進行準確預測，例如：從Facebook的按讚等紀錄中準確預測個人的私人信息(例如年齡、性別、個性等)。但機器學習並不是完全準確的，像是監督式的機器學習是透過經驗觀察去進行歸納，若給予的訓練資料中，包含的種類並不齊全，很有可能會使機器做出錯誤的判斷，因為無論是人還是機器都無法對於出現較少或從沒出現的東西做出準確的預測。例如：只給予白色天鵝的資料進行訓練，導致無法正確識別黑天鵝，因為機器認為”所有天鵝都是白色的”。另外，監督式機器學習在做出決策時會學習人類的偏見。常見的機器學習算法也傾向忽略少數群體來做到最大化多數群體的預測準確度。要避免此類問題的發生，可以透過平衡每個類別的訓練樣本數量，或是依照樣本數量給予權重。使用客觀的指標也有助於減少機器學習中一部份由人為所引起的偏差。

此篇文章中也討論了利用認知心理學與認知神經科學的方法了解偏誤決策的歷程。機器學習與人腦的決策過程都被稱作”黑盒子”，不過現今的神經科學方法在說明人腦與人工神經網路的內部運作上已有很大的躍進。神經科學中已有的許多技術也可以應用在人工神經網路上。像是腦損傷造成的特定方面行為損壞，使人們能夠確認大腦的哪個區域對哪個行為至關重要。同樣也可推廣到人工智能大腦中，了解機器學習中卷積神經網絡（CNN）對於感知處理的重要性。當無法使用神經科學方法時，仍可以運用心理學上的各種行為方法去間接推斷認知過程，以了解人工智慧如何做出可能帶有偏見的決策。例如：檢查特定族群中的獲勝者是否不成比例，來檢驗人工智慧選美” Beauty.AI”有無種族歧視的偏見。

隨著AI領域的擴大，機器越來越有能力做出與人類類似的判斷，但這些判斷並不一定是客觀的，也同樣受到決策偏見影響。期望在未來，我們能夠像理解人類的認知一樣，理解機器的認知！